{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-17T09:04:48.712323Z",
     "start_time": "2024-07-17T09:04:47.278035Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import jiant.utils.python.io as py_io\n",
    "import jiant.proj.simple.runscript as simple_run\n",
    "import jiant.scripts.download_data.runscript as downloader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T09:02:25.439375Z",
     "start_time": "2024-07-17T09:02:06.702277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tasks='superglue_winogender_diagnostics'\n",
    "\n",
    "downloader.download_data(tasks.split(','), f\"data\")\n",
    "\n",
    "args = simple_run.RunConfiguration(\n",
    "    run_name='bert-base-uncased-baseline-wino',\n",
    "    exp_dir='results/jiant',\n",
    "    data_dir='data',\n",
    "    hf_pretrained_model_name_or_path='bert-base-uncased',\n",
    "    tasks=tasks,\n",
    "    do_val=True,\n",
    "    #train_batch_size=16,\n",
    "    #num_train_epochs=1\n",
    ")\n",
    "simple_run.run_simple(args)"
   ],
   "id": "242d6073b335d847",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/30.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bd76e0bdc3f419bbe4463101d5cfc18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/18.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa258b6d36ac40efa079868b7649add8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d9e7df7fedb4977900ddabdea4c6603"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/356 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e8a35ada7bc41c4b84b4d2258e4ff62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and generated configs for 'superglue_winogender_diagnostics' (1/1)\n",
      "Tokenizing Task 'superglue_winogender_diagnostics' for phases 'test'\n",
      "SuperglueWinogenderDiagnosticsTask\n",
      "  [test]: /home/drechs13/Git/jiant/data/data/superglue_winogender_diagnostics/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing:   0%|          | 0/356 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e57399373377429192e1d4451ff56999"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Smart truncate chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56e3770dc9cf46f0acd34ebfb94a5817"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Smart truncate chunk-datum:   0%|          | 0/356 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e561af87f5d40b2bd9011528a1db667"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from start\n",
      "  jiant_task_container_config_path: results/jiant/run_configs/bert-base-uncased-baseline-wino_config.json\n",
      "  output_dir: results/jiant/runs/bert-base-uncased-baseline-wino\n",
      "  hf_pretrained_model_name_or_path: bert-base-uncased\n",
      "  model_path: results/jiant/models/bert-base-uncased/model/model.p\n",
      "  model_config_path: results/jiant/models/bert-base-uncased/model/config.json\n",
      "  model_load_mode: from_transformers\n",
      "  do_train: False\n",
      "  do_val: False\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: False\n",
      "  eval_every_steps: 0\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: False\n",
      "  seed: -1\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Using seed: 411604484\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"results/jiant/run_configs/bert-base-uncased-baseline-wino_config.json\",\n",
      "  \"output_dir\": \"results/jiant/runs/bert-base-uncased-baseline-wino\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"bert-base-uncased\",\n",
      "  \"model_path\": \"results/jiant/models/bert-base-uncased/model/model.p\",\n",
      "  \"model_config_path\": \"results/jiant/models/bert-base-uncased/model/config.json\",\n",
      "  \"model_load_mode\": \"from_transformers\",\n",
      "  \"do_train\": false,\n",
      "  \"do_val\": false,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": false,\n",
      "  \"eval_every_steps\": 0,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": false,\n",
      "  \"seed\": 411604484,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "1\n",
      "Creating Tasks:\n",
      "    superglue_winogender_diagnostics (SuperglueWinogenderDiagnosticsTask): data/configs/superglue_winogender_diagnostics_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drechs13/Git/jiant/jiant/proj/main/modeling/model_setup.py:181: UserWarning: The following weights were not loaded: dict_keys(['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias'])\n",
      "  warnings.warn(\n",
      "/home/drechs13/anaconda3/envs/jiant38/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No optimizer decay for:\n",
      "  encoder.embeddings.LayerNorm.weight\n",
      "  encoder.embeddings.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.attention.self.query.bias\n",
      "  encoder.encoder.layer.0.attention.self.key.bias\n",
      "  encoder.encoder.layer.0.attention.self.value.bias\n",
      "  encoder.encoder.layer.0.attention.output.dense.bias\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.intermediate.dense.bias\n",
      "  encoder.encoder.layer.0.output.dense.bias\n",
      "  encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.attention.self.query.bias\n",
      "  encoder.encoder.layer.1.attention.self.key.bias\n",
      "  encoder.encoder.layer.1.attention.self.value.bias\n",
      "  encoder.encoder.layer.1.attention.output.dense.bias\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.intermediate.dense.bias\n",
      "  encoder.encoder.layer.1.output.dense.bias\n",
      "  encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.attention.self.query.bias\n",
      "  encoder.encoder.layer.2.attention.self.key.bias\n",
      "  encoder.encoder.layer.2.attention.self.value.bias\n",
      "  encoder.encoder.layer.2.attention.output.dense.bias\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.intermediate.dense.bias\n",
      "  encoder.encoder.layer.2.output.dense.bias\n",
      "  encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.attention.self.query.bias\n",
      "  encoder.encoder.layer.3.attention.self.key.bias\n",
      "  encoder.encoder.layer.3.attention.self.value.bias\n",
      "  encoder.encoder.layer.3.attention.output.dense.bias\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.intermediate.dense.bias\n",
      "  encoder.encoder.layer.3.output.dense.bias\n",
      "  encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.attention.self.query.bias\n",
      "  encoder.encoder.layer.4.attention.self.key.bias\n",
      "  encoder.encoder.layer.4.attention.self.value.bias\n",
      "  encoder.encoder.layer.4.attention.output.dense.bias\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.intermediate.dense.bias\n",
      "  encoder.encoder.layer.4.output.dense.bias\n",
      "  encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.attention.self.query.bias\n",
      "  encoder.encoder.layer.5.attention.self.key.bias\n",
      "  encoder.encoder.layer.5.attention.self.value.bias\n",
      "  encoder.encoder.layer.5.attention.output.dense.bias\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.intermediate.dense.bias\n",
      "  encoder.encoder.layer.5.output.dense.bias\n",
      "  encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.attention.self.query.bias\n",
      "  encoder.encoder.layer.6.attention.self.key.bias\n",
      "  encoder.encoder.layer.6.attention.self.value.bias\n",
      "  encoder.encoder.layer.6.attention.output.dense.bias\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.intermediate.dense.bias\n",
      "  encoder.encoder.layer.6.output.dense.bias\n",
      "  encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.attention.self.query.bias\n",
      "  encoder.encoder.layer.7.attention.self.key.bias\n",
      "  encoder.encoder.layer.7.attention.self.value.bias\n",
      "  encoder.encoder.layer.7.attention.output.dense.bias\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.intermediate.dense.bias\n",
      "  encoder.encoder.layer.7.output.dense.bias\n",
      "  encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.attention.self.query.bias\n",
      "  encoder.encoder.layer.8.attention.self.key.bias\n",
      "  encoder.encoder.layer.8.attention.self.value.bias\n",
      "  encoder.encoder.layer.8.attention.output.dense.bias\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.intermediate.dense.bias\n",
      "  encoder.encoder.layer.8.output.dense.bias\n",
      "  encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.attention.self.query.bias\n",
      "  encoder.encoder.layer.9.attention.self.key.bias\n",
      "  encoder.encoder.layer.9.attention.self.value.bias\n",
      "  encoder.encoder.layer.9.attention.output.dense.bias\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.intermediate.dense.bias\n",
      "  encoder.encoder.layer.9.output.dense.bias\n",
      "  encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.attention.self.query.bias\n",
      "  encoder.encoder.layer.10.attention.self.key.bias\n",
      "  encoder.encoder.layer.10.attention.self.value.bias\n",
      "  encoder.encoder.layer.10.attention.output.dense.bias\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.intermediate.dense.bias\n",
      "  encoder.encoder.layer.10.output.dense.bias\n",
      "  encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.attention.self.query.bias\n",
      "  encoder.encoder.layer.11.attention.self.key.bias\n",
      "  encoder.encoder.layer.11.attention.self.value.bias\n",
      "  encoder.encoder.layer.11.attention.output.dense.bias\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.intermediate.dense.bias\n",
      "  encoder.encoder.layer.11.output.dense.bias\n",
      "  encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "  encoder.pooler.dense.bias\n",
      "  taskmodels_dict.superglue_winogender_diagnostics.head.dense.bias\n",
      "  taskmodels_dict.superglue_winogender_diagnostics.head.out_proj.bias\n",
      "Using AdamW\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T09:38:18.857361Z",
     "start_time": "2024-07-17T09:38:15.715483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import jiant.utils.python.io as py_io\n",
    "import jiant.proj.simple.runscript as run\n",
    "import jiant.scripts.download_data.runscript as downloader\n",
    "\n",
    "TASK_NAME = \"superglue_winogender_diagnostics\"\n",
    "\n",
    "HF_PRETRAINED_MODEL_NAME = \"bert-large-uncased\"\n",
    "\n",
    "MODEL_NAME = HF_PRETRAINED_MODEL_NAME.split(\"/\")[-1]\n",
    "RUN_NAME = f\"simple_{TASK_NAME}_{MODEL_NAME}\"\n",
    "EXP_DIR = \"exp\"\n",
    "DATA_DIR = \"exp/tasks\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "downloader.download_data([TASK_NAME], DATA_DIR)\n",
    "\n",
    "args = run.RunConfiguration(\n",
    "    run_name=RUN_NAME,\n",
    "    exp_dir=EXP_DIR,\n",
    "    data_dir=DATA_DIR,\n",
    "    hf_pretrained_model_name_or_path=HF_PRETRAINED_MODEL_NAME,\n",
    "    tasks=TASK_NAME,\n",
    "    train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    write_test_preds=True,\n",
    "    #write_val_preds=True,\n",
    ")\n",
    "run.run_simple(args)"
   ],
   "id": "6c355726b7a77d09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and generated configs for 'superglue_winogender_diagnostics' (1/1)\n",
      "Running from start\n",
      "  jiant_task_container_config_path: exp/run_configs/simple_superglue_winogender_diagnostics_bert-large-uncased_config.json\n",
      "  output_dir: exp/runs/simple_superglue_winogender_diagnostics_bert-large-uncased\n",
      "  hf_pretrained_model_name_or_path: bert-large-uncased\n",
      "  model_path: exp/models/bert-large-uncased/model/model.p\n",
      "  model_config_path: exp/models/bert-large-uncased/model/config.json\n",
      "  model_load_mode: from_transformers\n",
      "  do_train: False\n",
      "  do_val: False\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: True\n",
      "  eval_every_steps: 0\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: False\n",
      "  seed: -1\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Using seed: 2768048906\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"exp/run_configs/simple_superglue_winogender_diagnostics_bert-large-uncased_config.json\",\n",
      "  \"output_dir\": \"exp/runs/simple_superglue_winogender_diagnostics_bert-large-uncased\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"bert-large-uncased\",\n",
      "  \"model_path\": \"exp/models/bert-large-uncased/model/model.p\",\n",
      "  \"model_config_path\": \"exp/models/bert-large-uncased/model/config.json\",\n",
      "  \"model_load_mode\": \"from_transformers\",\n",
      "  \"do_train\": false,\n",
      "  \"do_val\": false,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": true,\n",
      "  \"eval_every_steps\": 0,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": false,\n",
      "  \"seed\": 2768048906,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "1\n",
      "Creating Tasks:\n",
      "    superglue_winogender_diagnostics (SuperglueWinogenderDiagnosticsTask): exp/tasks/configs/superglue_winogender_diagnostics_config.json\n",
      "No optimizer decay for:\n",
      "  encoder.embeddings.LayerNorm.weight\n",
      "  encoder.embeddings.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.attention.self.query.bias\n",
      "  encoder.encoder.layer.0.attention.self.key.bias\n",
      "  encoder.encoder.layer.0.attention.self.value.bias\n",
      "  encoder.encoder.layer.0.attention.output.dense.bias\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.intermediate.dense.bias\n",
      "  encoder.encoder.layer.0.output.dense.bias\n",
      "  encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.attention.self.query.bias\n",
      "  encoder.encoder.layer.1.attention.self.key.bias\n",
      "  encoder.encoder.layer.1.attention.self.value.bias\n",
      "  encoder.encoder.layer.1.attention.output.dense.bias\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.intermediate.dense.bias\n",
      "  encoder.encoder.layer.1.output.dense.bias\n",
      "  encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.attention.self.query.bias\n",
      "  encoder.encoder.layer.2.attention.self.key.bias\n",
      "  encoder.encoder.layer.2.attention.self.value.bias\n",
      "  encoder.encoder.layer.2.attention.output.dense.bias\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.intermediate.dense.bias\n",
      "  encoder.encoder.layer.2.output.dense.bias\n",
      "  encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.attention.self.query.bias\n",
      "  encoder.encoder.layer.3.attention.self.key.bias\n",
      "  encoder.encoder.layer.3.attention.self.value.bias\n",
      "  encoder.encoder.layer.3.attention.output.dense.bias\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.intermediate.dense.bias\n",
      "  encoder.encoder.layer.3.output.dense.bias\n",
      "  encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.attention.self.query.bias\n",
      "  encoder.encoder.layer.4.attention.self.key.bias\n",
      "  encoder.encoder.layer.4.attention.self.value.bias\n",
      "  encoder.encoder.layer.4.attention.output.dense.bias\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.intermediate.dense.bias\n",
      "  encoder.encoder.layer.4.output.dense.bias\n",
      "  encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.attention.self.query.bias\n",
      "  encoder.encoder.layer.5.attention.self.key.bias\n",
      "  encoder.encoder.layer.5.attention.self.value.bias\n",
      "  encoder.encoder.layer.5.attention.output.dense.bias\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.intermediate.dense.bias\n",
      "  encoder.encoder.layer.5.output.dense.bias\n",
      "  encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.attention.self.query.bias\n",
      "  encoder.encoder.layer.6.attention.self.key.bias\n",
      "  encoder.encoder.layer.6.attention.self.value.bias\n",
      "  encoder.encoder.layer.6.attention.output.dense.bias\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.intermediate.dense.bias\n",
      "  encoder.encoder.layer.6.output.dense.bias\n",
      "  encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.attention.self.query.bias\n",
      "  encoder.encoder.layer.7.attention.self.key.bias\n",
      "  encoder.encoder.layer.7.attention.self.value.bias\n",
      "  encoder.encoder.layer.7.attention.output.dense.bias\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.intermediate.dense.bias\n",
      "  encoder.encoder.layer.7.output.dense.bias\n",
      "  encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.attention.self.query.bias\n",
      "  encoder.encoder.layer.8.attention.self.key.bias\n",
      "  encoder.encoder.layer.8.attention.self.value.bias\n",
      "  encoder.encoder.layer.8.attention.output.dense.bias\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.intermediate.dense.bias\n",
      "  encoder.encoder.layer.8.output.dense.bias\n",
      "  encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.attention.self.query.bias\n",
      "  encoder.encoder.layer.9.attention.self.key.bias\n",
      "  encoder.encoder.layer.9.attention.self.value.bias\n",
      "  encoder.encoder.layer.9.attention.output.dense.bias\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.intermediate.dense.bias\n",
      "  encoder.encoder.layer.9.output.dense.bias\n",
      "  encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.attention.self.query.bias\n",
      "  encoder.encoder.layer.10.attention.self.key.bias\n",
      "  encoder.encoder.layer.10.attention.self.value.bias\n",
      "  encoder.encoder.layer.10.attention.output.dense.bias\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.intermediate.dense.bias\n",
      "  encoder.encoder.layer.10.output.dense.bias\n",
      "  encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.attention.self.query.bias\n",
      "  encoder.encoder.layer.11.attention.self.key.bias\n",
      "  encoder.encoder.layer.11.attention.self.value.bias\n",
      "  encoder.encoder.layer.11.attention.output.dense.bias\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.intermediate.dense.bias\n",
      "  encoder.encoder.layer.11.output.dense.bias\n",
      "  encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.12.attention.self.query.bias\n",
      "  encoder.encoder.layer.12.attention.self.key.bias\n",
      "  encoder.encoder.layer.12.attention.self.value.bias\n",
      "  encoder.encoder.layer.12.attention.output.dense.bias\n",
      "  encoder.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.12.intermediate.dense.bias\n",
      "  encoder.encoder.layer.12.output.dense.bias\n",
      "  encoder.encoder.layer.12.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.12.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.13.attention.self.query.bias\n",
      "  encoder.encoder.layer.13.attention.self.key.bias\n",
      "  encoder.encoder.layer.13.attention.self.value.bias\n",
      "  encoder.encoder.layer.13.attention.output.dense.bias\n",
      "  encoder.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.13.intermediate.dense.bias\n",
      "  encoder.encoder.layer.13.output.dense.bias\n",
      "  encoder.encoder.layer.13.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.13.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.14.attention.self.query.bias\n",
      "  encoder.encoder.layer.14.attention.self.key.bias\n",
      "  encoder.encoder.layer.14.attention.self.value.bias\n",
      "  encoder.encoder.layer.14.attention.output.dense.bias\n",
      "  encoder.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.14.intermediate.dense.bias\n",
      "  encoder.encoder.layer.14.output.dense.bias\n",
      "  encoder.encoder.layer.14.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.14.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.15.attention.self.query.bias\n",
      "  encoder.encoder.layer.15.attention.self.key.bias\n",
      "  encoder.encoder.layer.15.attention.self.value.bias\n",
      "  encoder.encoder.layer.15.attention.output.dense.bias\n",
      "  encoder.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.15.intermediate.dense.bias\n",
      "  encoder.encoder.layer.15.output.dense.bias\n",
      "  encoder.encoder.layer.15.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.15.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.16.attention.self.query.bias\n",
      "  encoder.encoder.layer.16.attention.self.key.bias\n",
      "  encoder.encoder.layer.16.attention.self.value.bias\n",
      "  encoder.encoder.layer.16.attention.output.dense.bias\n",
      "  encoder.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.16.intermediate.dense.bias\n",
      "  encoder.encoder.layer.16.output.dense.bias\n",
      "  encoder.encoder.layer.16.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.16.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.17.attention.self.query.bias\n",
      "  encoder.encoder.layer.17.attention.self.key.bias\n",
      "  encoder.encoder.layer.17.attention.self.value.bias\n",
      "  encoder.encoder.layer.17.attention.output.dense.bias\n",
      "  encoder.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.17.intermediate.dense.bias\n",
      "  encoder.encoder.layer.17.output.dense.bias\n",
      "  encoder.encoder.layer.17.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.17.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.18.attention.self.query.bias\n",
      "  encoder.encoder.layer.18.attention.self.key.bias\n",
      "  encoder.encoder.layer.18.attention.self.value.bias\n",
      "  encoder.encoder.layer.18.attention.output.dense.bias\n",
      "  encoder.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.18.intermediate.dense.bias\n",
      "  encoder.encoder.layer.18.output.dense.bias\n",
      "  encoder.encoder.layer.18.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.18.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.19.attention.self.query.bias\n",
      "  encoder.encoder.layer.19.attention.self.key.bias\n",
      "  encoder.encoder.layer.19.attention.self.value.bias\n",
      "  encoder.encoder.layer.19.attention.output.dense.bias\n",
      "  encoder.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.19.intermediate.dense.bias\n",
      "  encoder.encoder.layer.19.output.dense.bias\n",
      "  encoder.encoder.layer.19.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.19.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.20.attention.self.query.bias\n",
      "  encoder.encoder.layer.20.attention.self.key.bias\n",
      "  encoder.encoder.layer.20.attention.self.value.bias\n",
      "  encoder.encoder.layer.20.attention.output.dense.bias\n",
      "  encoder.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.20.intermediate.dense.bias\n",
      "  encoder.encoder.layer.20.output.dense.bias\n",
      "  encoder.encoder.layer.20.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.20.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.21.attention.self.query.bias\n",
      "  encoder.encoder.layer.21.attention.self.key.bias\n",
      "  encoder.encoder.layer.21.attention.self.value.bias\n",
      "  encoder.encoder.layer.21.attention.output.dense.bias\n",
      "  encoder.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.21.intermediate.dense.bias\n",
      "  encoder.encoder.layer.21.output.dense.bias\n",
      "  encoder.encoder.layer.21.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.21.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.22.attention.self.query.bias\n",
      "  encoder.encoder.layer.22.attention.self.key.bias\n",
      "  encoder.encoder.layer.22.attention.self.value.bias\n",
      "  encoder.encoder.layer.22.attention.output.dense.bias\n",
      "  encoder.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.22.intermediate.dense.bias\n",
      "  encoder.encoder.layer.22.output.dense.bias\n",
      "  encoder.encoder.layer.22.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.22.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.23.attention.self.query.bias\n",
      "  encoder.encoder.layer.23.attention.self.key.bias\n",
      "  encoder.encoder.layer.23.attention.self.value.bias\n",
      "  encoder.encoder.layer.23.attention.output.dense.bias\n",
      "  encoder.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.23.intermediate.dense.bias\n",
      "  encoder.encoder.layer.23.output.dense.bias\n",
      "  encoder.encoder.layer.23.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.23.output.LayerNorm.bias\n",
      "  encoder.pooler.dense.bias\n",
      "  taskmodels_dict.superglue_winogender_diagnostics.head.dense.bias\n",
      "  taskmodels_dict.superglue_winogender_diagnostics.head.out_proj.bias\n",
      "Using AdamW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Eval (superglue_winogender_diagnostics, Test):   0%|          | 0/12 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a580829317bd4f8dad1bbd71c1a9eefb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
